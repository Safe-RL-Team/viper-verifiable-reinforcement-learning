<!doctype html>

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <style id="distill-article-specific-styles">
        <%=require("../static/styles.css") %>
    </style>
    <script src="https://distill.pub/template.v2.js"></script>
</head>

<body>

<d-front-matter>
    <script type="text/json">
      <%= JSON.stringify(require("./frontmatter.json"), null, 4) %>
























    </script>
</d-front-matter>

<d-title>
    <h1>Viper</h1>
    <p>Verifiable Reinforcement Learning via Policy Extraction</p>
</d-title>

<d-article>

    <p>
        While deep reinforcement learning has been shown to be effective in a variety of domains, it suffers from
        the same problems as other machine learning methods: it is difficult to interpret and it is difficult to
        guarantee that it is behaving correctly. In this article I present my efforts in reproducing the results of the
        paper
        "Viper: Verifiable Reinforcement Learning via Policy Extraction"
        <d-cite key="viper"></d-cite>
        by <a href="https://obastani.github.io/">Osbert Bastani</a> et al.

        In particular, I will answer the following questions with a focus on actual implementation rather than theory:
    <ul>
        <li>How to distill a Deep RL agent into a decision tree?</li>
        <li>How to verify that it never loses?</li>
    </ul>
    </p>

    <p>The code accompanying this article can be found <a
                href="https://github.com/Safe-RL-Team/viper-verifiable-rl-impl">here</a>.</p>

    <h2>Background</h2>

    <p>
        As Reinforcement Learning is increasingly used in safety-critical domains, it is important to be able to
        verify that the agent is behaving correctly. This is especially important in domains where the agent is
        interacting with the real world, such as self-driving cars, air traffic control, and robotics. The difference
        between
        these applications and say, playing a game of Go, is that in the real-world scenarios the cost of failure is
        much higher perhaps even unacceptable. So while there are approaches that aim to teach RL agents safe behavior
        by for instance [TODO add and cite examples] we would optimally like to be able to demonstrate that our agent
        never fails.
    </p>
    <p>
        Even though there is work
        <d-cite key="katz"></d-cite>
        on verifying Deep Reinforcement Learning agents the process is complicated
        by the complexity of the computation graph of a neural network. Instead, we will first train a Deep RL agent and
        then distill it into a decision tree (DT) using imitation learning. This decision tree can then be verified
        using a SAT solver.
    </p>

    <h2>Training the oracle</h2>

    <p>
        As we will see later in order for our verification algorithm to work we need to have a functional description of
        the state dynamics f(s) of our environment. While the authors are also able to verify other properties such as
        robustness on environments where that is not the case such as Atari Pong, we will here only focus on the <code>Toy
            Pong</code> environment where we can easily compute the state dynamics.
    </p>

    <p>
        The environment is a one player version of the Atari Pong where the player controls a paddle and the opponent is
        simply a wall. The paddle can move left or right and the ball bounces off the walls and the paddle.
        The goal is to keep the ball in play for as long as possible.
        We can describe the state of the environment at timestep t with the following vector:
    </p>

    <d-math block=""> s_t = (x_p, x_b, y_b, v_x, v_y)</d-math>
    <ul>
        <li>
            <d-math>x_p</d-math>
            horizontal position of the paddle
        </li>
        <li>
            <d-math>x_b</d-math>
            x coordinate of the ball
        </li>
        <li>
            <d-math>y_b</d-math>
            y coordinate of the ball
        </li>
        <li>
            <d-math>v_x</d-math>
            x velocity of the ball
        </li>
        <li>
            <d-math>v_y</d-math>
            y velocity of the ball
        </li>
    </ul>

    <figure>
        <img class="figure-content" src="images/toy_pong_init.png" style="width: 50%">
        <figcaption class="caption-centered">
            The ToyPong environment at timestep 0
        </figcaption>
    </figure>
    <p>
        The ball starts at the center of the screen and is initialized with a random velocity
        in both the x and the y direction with a magnitude between 1 and 2. The paddle starts at the center bottom of
        the
        screen and can be controlled via the actions <code>{left, right, stay}</code>.
        The speed of the paddle is not given so I initially assumed it to be 1. However, after some experimentation I
        quickly learned that this makes the game impossible to play perfectly as there are many cases where the ball is
        too
        fast to catch no matter what the controller does. A quick calculation shows that the paddle's speed needs to be
        at least 1.5.
    </p>

    <figure>
        <img class="figure-content" src="images/toy_pong_impossible.gif" style="width: 40%">
    </figure>

    <p>
        Having the <a href="https://github.com/Safe-RL-Team/viper-verifiable-rl-impl/blob/main/gym_env/toy_pong.py">environment
            setup</a> we now would like to train an oracle that can play the game perfectly. While the
        authors used a generic policy gradient
        for training a DNN policy I decided to go directly for the stable baselines 3 <a
                href="https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html">PPO</a> implementation as it
        is a good starter for Deep RL. Usually it is enough in RL for an agent to achieve a very high reward so training
        even this state-of-the-art algorithm to play <i>perfectly</i>
        <d-footnote id="d-footnote-1">Here perfectly only means that the agent achieves the highest reward possible
            (r=250) averaged over 50 rollouts. Unlike the decision tree we are extracting it might still lose given
            certain edge cases.
        </d-footnote>
        turned out to be surprisingly difficult. The key hyperparameter that made it work in the end was using a
        learning rate that would decay linearly after half the training was complete.
    </p>

    <h2>üêç VIPER</h2>

    <p>
        One naive way of approaching the distillation challenge would be to train our decision tree to predict the
        action of the oracle given the states of all trajectories in the training set. This, however, turns out to yield
        poor
        results because the decision tree is not able to generalize to unseen states. This is because mo matter how good
        our
        action classification accuracy is on the training set, the decision tree will likely end up
        in a state where it has never seen before. This is especially true for stochastic environments. DAgger
        <d-cite
                key="dagger"></d-cite>
        is a method that addresses this problem by letting the decision tree play on its own
        and then retrieving the actions from the oracle and adding them to the training set. This process is repeated
        until the decision tree is able to play the game perfectly.
    </p>

    <p>The VIPER algorithm is based on DAgger but adds one important insight: Not all states in a game are equally
        important its outcome.
        In the figure below for instance we can see that on the left side the ball is moving away from the paddle so
        the next action is not consequential for the overall return. On the right side however it is crucial for the
        paddle to move to the right in order to keep the ball in play. We can use this insight to weight state action
        pairs
        by their importance for the overall return. But how do we know which states are critical and which are not?
    </p>

    <figure>
        <img class="figure-content" src="images/critical_states.png">
        <figcaption class="caption-centered">
            The state on the left is not critical for the overall return while the state on the right is. Adapted from
            Bastani et al.
            <d-cite key="viper"></d-cite>
        </figcaption>
    </figure>

    <p>
        For this we can leverage the Q-function of the oracle
        <d-math>\pi^*</d-math>
        . Remember: The Q-function is a function that maps a state
        and an
        action to the expected return of taking that action in that state. Now imagine suppose we have a set of q-values
        for a given state and the best possible action has a q-value that is very close to the worst possible action. In
        that case we can be pretty sure that the best action is not critical for the overall return. On the other hand
        if
        the difference is very high then making the wrong choice could have disastrous consequences. We can now weight
        every single sample in our training set using this loss:
    </p>

    <style>
        .eq-grid {
            display: grid;
            justify-content: start;
            grid-row-gap: 10px;
        }

        .eq-grid figcaption d-math {
            font-size: 100%;
        }

        .eq-grid .expansion-marker {
            border: 1px solid #CCC;
            border-bottom: none;
            height: .5em;
            width: 100%;
        }
    </style>

    <figure class="eq-grid">

        <figcaption style="grid-row: 1; grid-column: 2/5; text-align: center">
            The "criticalness" of the state.
        </figcaption>

        <div class="expansion-marker" style="grid-row: 2; grid-column: 2 / 5;"></div>

        <div style="grid-row: 3; grid-column: 1;">
            <d-math> \tilde{\ell}(s_t) ~~~=~~~~</d-math>
        </div>
        <div style="grid-row: 3; grid-column: 2;">
            <d-math>[V_t^{(\pi^*)}</d-math>
        </div>
        <div style="grid-row: 3; grid-column: 3;">
            <d-math> ~~~-~~~</d-math>
        </div>
        <div style="grid-row: 3; grid-column: 4;">
            <d-math> \min_{a \in A}Q_t^{(\pi^*)}(s, a)]</d-math>
        </div>
        <div style="grid-row: 3; grid-column: 5;">
            <d-math>~~~ \mathbb{I}\left[\pi(s) \neq \pi^*(s)\right]</d-math>
        </div>


        <figcaption style="grid-row: 4; grid-column: 2; max-width:135px; ">
            The maximum q-value
        </figcaption>
        <figcaption style="grid-row: 4; grid-column: 4; max-width:120px;">
            the minimum q-value
        </figcaption>
        <figcaption style="grid-row: 4; grid-column: 5; max-width:120px; text-align: center">
            the classifier 0-1 loss
        </figcaption>

    </figure>

    <p>The sample weights can be obtained directly from our stable baselines oracle policy <a
                href="https://github.com/Safe-RL-Team/viper-verifiable-rl-impl/blob/main/train/viper.py#L114">like
            so</a>. Now we have all the components to build and analyze the VIPER algorithm. The code below is a
        simplified snippet from the <a
                href="https://github.com/Safe-RL-Team/viper-verifiable-rl-impl/blob/main/train/viper.py#L19">repository</a>
        with comments to explain each step:
    </p>

    <d-code block="" language="python">
        dataset = []
        policy = None
        policies = []
        rewards = []

        for i in range(args.n_iter):
            # Beta controls if we use the oracle or the decision tree
            # Use the
            beta = 1 if i == 0 else 0
            dataset += sample_trajectory(args, policy, beta)

            # Train a scikit-learn decision tree on the growing dataset
            clf = DecisionTreeClassifier(ccp_alpha=0.0001,
                                         criterion="entropy", max_depth=args.max_depth,
                                         max_leaf_nodes=args.max_leaves)
            x = np.array([traj[0] for traj in dataset])
            y = np.array([traj[1] for traj in dataset])
            weight = np.array([traj[2] for traj in dataset])

            clf.fit(x, y, sample_weight=weight)

            # The current policy is the one that will be used to sample the next trajectory
            policy = clf
            policies.append(clf)

            mean_reward, std_reward = evaluate_policy(policy, env, n_eval_episodes=100)
            if args.verbose == 2:
                print(f"Policy score: {mean_reward:0.4f} +/- {std_reward:0.4f}")
            rewards.append(mean_reward)

        # Retain the best policy over all runs
        print(f"Viper iteration complete. Dataset size: {len(dataset)}")
        best_policy = policies[np.argmax(rewards)]
        path = get_viper_path(args)
        print(f"Best policy:\t{np.argmax(rewards)}")
        print(f"Mean reward:\t{np.max(rewards):0.4f}")
    </d-code>

    <p>While I was able to train a decision tree to play perfectly on both CartPole and ToyPong there are a few things
        to note:</p>

    <ul>
        <li>The only specification w.r.t. to the the tree classifier in the paper is that they use the CART algorithm. I
            found that the default gini coefficient performs slightly worse than splitting based on entropy.
        </li>
        <li>For ToyPong my tree ended up much larger than the one reported by the authors (587 vs 61 leaf nodes). I only
            later realized that in their setup the x position of the ball is likely randomized which would lead to a
            much more diverse dataset and thus prevents overfitting.
        </li>
        <li>Pruning helps limit the tree size and can even improve performance as it regularizes the tree.</li>
    </ul>

    <h3>Differences to DAgger</h3>

    <p>VIPER extends DAgger with the q-sampling trick but it also changes the training schedule by only letting the
        oracle play in the first iteration while DAgger gradually reduces the participation of the oracle in each run.
        It is thus very easy to modify the above code to work like DAgger and I was able to verify that both changes
        lead to a better performance.
    </p>

    <h2>Verifying Correctness</h2>

    <p>We now have a DNN policy and a decision tree that both achieve a maximum reward on our ToyPong game. Now we want
        to verify that there is no edge case that makes </p>

    <h2>Reflection</h2>

</d-article>


<d-appendix>

    <d-footnote-list></d-footnote-list>
    <d-citation-list></d-citation-list>
</d-appendix>

<!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
<d-bibliography src="bibliography.bib"></d-bibliography>

</body>
