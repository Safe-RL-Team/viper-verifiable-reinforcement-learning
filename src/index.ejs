<!doctype html>

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <style id="distill-article-specific-styles">
        <%=require("../static/styles.css") %>
    </style>
    <script src="https://distill.pub/template.v2.js"></script>
</head>

<body>

<d-front-matter>
    <script type="text/json">
      <%= JSON.stringify(require("./frontmatter.json"), null, 4) %>





















    </script>
</d-front-matter>

<d-title>
    <h1>Viper</h1>
    <p>Verifiable Reinforcement Learning via Policy Extraction</p>
</d-title>

<d-article>

    <p>
        While deep reinforcement learning has been shown to be effective in a variety of domains, it suffers from
        the same problems as other machine learning methods: it is difficult to interpret and it is difficult to
        guarantee that it is behaving correctly. In this article, I present my efforts in reproducing the results of the
        paper "Verifiable Reinforcement Learning via Policy Extraction"
        <d-cite key="viper"></d-cite>
        by <a href="https://obastani.github.io/">Osbert Bastani</a> et al.
        In particular, I will answer the following questions:
    <ul>
        <li><a href="#viper">How to distill a Deep RL agent into a decision tree?<a></a></li>
        <li><a href="#verification">How to verify that it never loses?</a></li>
    </ul>
    </p>

    <p>The code accompanying this article can be found <a
                href="https://github.com/Safe-RL-Team/viper-verifiable-rl-impl">here</a>.</p>

    <h2>Background</h2>

    <p>
        Reinforcement Learning is gaining increasing importance in safety-critical domains, necessitating the ability to
        verify that the agent is behaving correctly. This is especially important in domains where the agent is
        interacting with the real world, such as self-driving cars, air traffic control, and robotics. Unlike playing a
        game of Go, failure in these real-world scenarios can incur significantly higher perhaps even unacceptable
        costs. So while there are approaches that aim to teach RL agents safe behavior we would optimally like to be
        able to demonstrate that our agent never fails.
    </p>
    <p>
        Even though there is work
        <d-cite key="katz"></d-cite>
        on verifying Deep Reinforcement Learning agents the process is complicated
        by the complexity of the computation graph of a neural network. Instead, we will first train a Deep RL agent and
        then distill it into a decision tree (DT) using imitation learning. This decision tree can then be verified
        using an SAT solver.
    </p>

    <h2>Training the oracle</h2>

    <p>
        As we will see later for our verification algorithm to work we need to have a functional description of
        the state dynamics f(s) of our environment. While the authors are also able to verify other properties such as
        robustness in environments where that is not the case such as Atari Pong, we will here only focus on the <code>Toy
            Pong</code> environment where we can easily compute the state dynamics.
    </p>

    <p>
        The environment is a one-player version of the Atari Pong where the player controls a paddle and the opponent is
        simply a wall. The paddle can move left or right and the ball bounces off the walls and the paddle.
        The goal is to keep the ball in play for as long as possible.
        We can describe the state of the environment at timestep t with the following vector:
    </p>

    <d-math block=""> s_t = (x_p, x_b, y_b, v_x, v_y)</d-math>
    <ul>
        <li>
            <d-math>x_p</d-math>
            horizontal position of the paddle
        </li>
        <li>
            <d-math>x_b</d-math>
            x coordinate of the ball
        </li>
        <li>
            <d-math>y_b</d-math>
            y coordinate of the ball
        </li>
        <li>
            <d-math>v_x</d-math>
            x velocity of the ball
        </li>
        <li>
            <d-math>v_y</d-math>
            y velocity of the ball
        </li>
    </ul>

    <figure>
        <img class="figure-content" src="images/toy_pong_init.png" style="width: 50%">
        <figcaption class="caption-centered">
            The ToyPong environment at timestep 0
        </figcaption>
    </figure>
    <p>
        The ball starts at the center of the screen and is initialized with a random velocity
        in both the x and the y direction with a magnitude between 1 and 2. The paddle starts at the center bottom of
        the
        screen and can be controlled via the actions <code>{left, right, stay}</code>.
        The speed of the paddle is not given, so I initially assumed it to be 1. However, after some experimentation, I
        quickly learned that this makes the game impossible to play perfectly as there are many cases where the ball is
        too
        fast to catch no matter what the controller does. A quick calculation shows that the paddle's speed needs to be
        at least 1.5.
    </p>

    <figure>
        <img class="figure-content" src="images/toy_pong_impossible.gif" style="width: 40%">
    </figure>

    <p>
        Having the <a href="https://github.com/Safe-RL-Team/viper-verifiable-rl-impl/blob/main/gym_env/toy_pong.py">environment
        </a> set up we now would like to train an oracle that can play the game perfectly. While the
        authors used a generic policy gradient
        for training a DNN policy I decided to go directly for the stable baselines 3 <a
                href="https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html">PPO</a> implementation as it
        is a good starter for Deep RL. Usually it is enough in RL for an agent to achieve a very high reward so training
        even this state-of-the-art algorithm to play <i>perfectly</i>
        <d-footnote id="d-footnote-1">Here perfectly only means that the agent achieves the highest reward possible
            (r=250) averaged over 50 rollouts. Unlike the decision tree we are extracting it might still lose given
            certain edge cases.
        </d-footnote>
        turned out to be surprisingly difficult. The key hyperparameter that made it work in the end was using a
        learning rate that would decay linearly after half the training was complete.
    </p>

    <h2 id="viper">üêç VIPER</h2>

    <p>
        One naive way of approaching the distillation challenge would be to train our decision tree to predict the
        action of the oracle given the states of all trajectories in the training set. This, however, turns out to yield
        poor
        results because the decision tree is not able to generalize to unseen states. This is because mo matter how good
        our
        action classification accuracy is on the training set, the decision tree will likely end up
        in a state where it has never seen before. This is especially true for stochastic environments. DAgger
        <d-cite
                key="dagger"></d-cite>
        is an algorithm that solves this by letting the decision tree play on its own
        and then retrieving the correct actions from the oracle and adding them to the training set. This process is
        repeated
        as often as necessary yielding an ever expanding training set.
    </p>

    <p>The VIPER algorithm is based on DAgger but adds one important insight: Not all states in a game are equally
        important to its outcome.
        In the figure below for instance we can see that on the left side, the ball is moving away from the paddle so
        the next action is not consequential for the overall return. On the right side however it is crucial for the
        paddle to move to the right in order to keep the ball in play. We can use this insight to weight state action
        pairs
        by their importance for the overall return. But how do we know which states are critical and which are not?
    </p>

    <figure>
        <img class="figure-content" src="images/critical_states.png">
        <figcaption class="caption-centered">
            The state on the left is not critical for the overall return while the state on the right is. Adapted
            from
            Bastani et al.
            <d-cite key="viper"></d-cite>
        </figcaption>
    </figure>

    <p>
        For this, we can leverage the Q-function of the oracle
        <d-math>\pi^*</d-math>
        . Remember: The Q-function is a function that maps a state
        and an action to the expected return of taking that action in that state. Now suppose we have a set of q-values
        for a given state and the best possible action has a q-value that is very close to the worst possible action. In
        that case, we can be pretty sure that the best action is not critical for the overall return because no matter
        which action we pick the expected return will not change much. On the other hand if the difference is very high
        then
        making the wrong choice could have disastrous consequences. We can now use this to weight every sample in
        our training set with this expression:
    </p>

    <style>
        .eq-grid {
            display: grid;
            justify-content: start;
            grid-row-gap: 10px;
        }

        .eq-grid figcaption d-math {
            font-size: 100%;
        }

        .eq-grid .expansion-marker {
            border: 1px solid #CCC;
            border-bottom: none;
            height: .5em;
            width: 100%;
        }
    </style>

    <figure class="eq-grid">

        <figcaption style="grid-row: 1; grid-column: 2/5; text-align: center">
            The "criticalness" of the state.
        </figcaption>

        <div class="expansion-marker" style="grid-row: 2; grid-column: 2 / 5;"></div>

        <div style="grid-row: 3; grid-column: 1;">
            <d-math> \tilde{\ell}(s_t) ~~~=~~~~</d-math>
        </div>
        <div style="grid-row: 3; grid-column: 2;">
            <d-math>[V_t^{(\pi^*)}</d-math>
        </div>
        <div style="grid-row: 3; grid-column: 3;">
            <d-math> ~~~-~~~</d-math>
        </div>
        <div style="grid-row: 3; grid-column: 4;">
            <d-math> \min_{a \in A}Q_t^{(\pi^*)}(s, a)]</d-math>
        </div>
        <div style="grid-row: 3; grid-column: 5;">
            <d-math>~~~ \mathbb{I}\left[\pi(s) \neq \pi^*(s)\right]</d-math>
        </div>


        <figcaption style="grid-row: 4; grid-column: 2; max-width:135px; ">
            The maximum q-value
        </figcaption>
        <figcaption style="grid-row: 4; grid-column: 4; max-width:120px;">
            the minimum q-value
        </figcaption>
        <figcaption style="grid-row: 4; grid-column: 5; max-width:120px; text-align: center">
            the classifier 0-1 loss
        </figcaption>

    </figure>

    <p>The sample weights can be obtained directly from our stable baselines oracle policy <a
                href="https://github.com/Safe-RL-Team/viper-verifiable-rl-impl/blob/main/train/viper.py#L114">like
            so</a>. Now we have all the components to build and analyze the VIPER algorithm. The code below is a
        simplified snippet from the <a
                href="https://github.com/Safe-RL-Team/viper-verifiable-rl-impl/blob/main/train/viper.py#L19">repository</a>
        with comments to explain each step:
    </p>

    <d-code block="" language="python">
        dataset = []
        policy = None
        policies = []
        rewards = []

        for i in range(args.n_iter):
        # Beta controls if we use the oracle or the decision tree
        # Use the oracle only for the first iteration
        beta = 1 if i == 0 else 0
        dataset += sample_trajectory(args, policy, beta)

        # Train a scikit-learn decision tree on the growing dataset
        clf = DecisionTreeClassifier(ccp_alpha=0.0001,
        criterion="entropy",
        max_depth=args.max_depth,
        max_leaf_nodes=args.max_leaves)
        x = np.array([traj[0] for traj in dataset])
        y = np.array([traj[1] for traj in dataset])
        weight = np.array([traj[2] for traj in dataset])

        clf.fit(x, y, sample_weight=weight)

        # The current policy is the one that will
        # be used to sample the next trajectory
        policy = clf
        policies.append(clf)

        mean_reward = evaluate_policy(policy, env)
        rewards.append(mean_reward)

        # Retain the best policy over all runs
        best_policy = policies[np.argmax(rewards)]
        path = get_viper_path(args)
        print(f"Best policy:\t{np.argmax(rewards)}")
        print(f"Mean reward:\t{np.max(rewards):0.4f}")
    </d-code>

    <p>While I was able to train a decision tree to play perfectly on both CartPole and ToyPong there are a few things
        to note:</p>

    <ul>
        <li>The only specification w.r.t. to the tree classifier in the paper is that they use the CART algorithm. I
            found the default Gini coefficient to perform slightly worse than splitting based on entropy.
        </li>
        <li>For ToyPong my tree ended up much larger than the one reported by the authors (587 vs 61 leaf nodes). I only
            later realized that in their setup the x position of the ball is likely randomized which would lead to a
            much more diverse dataset and thus prevents overfitting.
        </li>
        <li>Pruning helps limit the tree size and can even improve performance as it regularizes the tree.</li>
    </ul>

    <h3>Differences to DAgger</h3>

    <p>VIPER extends DAgger with the q-sampling trick, but it also changes the training schedule by only letting the
        oracle play in the first iteration while DAgger gradually reduces the participation of the oracle in each run.
        It is thus very easy to modify the above code to work like DAgger, and I was able to verify that both changes
        lead to better performance.
    </p>

    <h2 id="verification">Verifying Correctness</h2>

    <p>We now have a DNN policy and a decision tree that both achieve a maximum reward on our ToyPong game. Now we want
        to verify that there is no edge case that would make our controller lose. This was the hardest part of the
        implementation
        and to show why it will help to have a look at the relevant section of the paper:
    </p>

    <figure>
        <img class="figure-content" src="images/verification_paper.png">
        <figcaption class="caption-centered">
            Excerpt from the paper
            <d-cite key="viper"></d-cite>
        </figcaption>
    </figure>

    <p>
        So the idea is that because the joint dynamics $f_{\pi}(s)$ are piecewise linear function we can use the <a
                href="https://en.wikipedia.org/wiki/Linear_programming">linear programming</a> algorithm to find
        exceptions to an equation that says that after at most $ t_{max} $ time steps we always end up in a safe state $
        Y_0 $. To better understand the point about the piecewise linearity you can look at the figure below. Decision
        trees partition their input space into regions and each region is associated with a leaf node, i.e. action. The
        same can be said for the state dynamics which would for instance tell you that within the box the ball keeps
        moving while at each wall it bounces back. So if you come up with the right partitioning you
        can essentially write the joint dynamics as $f_{\pi}(s) = \beta_i^T s$ where each $\beta_i$ is a vector that
        encodes the next state of the system using the controller and system dynamics. Each $ \phi_t $ in the above
        program then captures the fact that if we are in one of the state partitions the transition to the next state
        will
        be governed by the corresponding $\beta_i$
        <d-footnote id="d-footnote-1">A sharp-eyed reader might have already spotted
            a mistake in the $ \phi_t $ expression: While the paper specifies a disjunction over the state partitions
            this would
            make the expression <b>trivially true</b> because we are already not in all states but one. Instead, it has
            to be a conjunction which my experiments confirm.
        </d-footnote>
        .
    </p>

    <figure>
        <img class="figure-content" src="images/joint_dynamics.png">
        <figcaption class="caption-centered">
            If the state dynamics are piecewise linear we can write the joint dynamics as a linear combination of the
            dynamics and the predicted action for each leaf node
            <d-cite key="viper"></d-cite>
        </figcaption>
    </figure>

    <p><b>But how do you programmatically build the correct partition and $ \beta $ from the decision tree?</b> At first
        this question seemed stunningly hard until I realized that there is a straight-forward solution that was
        probably
        cut from the paper since it would have only complicated the equations. The trick is to split each expression in
        $ \phi_t $ into a controller and a system part. The controller part only controls $ s_t[0] $, i.e. the paddle,
        and the system equations, which have to be specified manually, the rest.
    </p>

    <p>
        The full correctness check then comes out at less than 300 lines of code with comments. It uses the theorem
        prover
        <a href="https://github.com/Z3Prover/z3">z3</a> to show that the $ \neg \psi $ is unsatisfiable, i.e. there is
        no counter example to the safety property. Since my decision tree is considerably larger than the one in the
        paper
        the check takes about 38 seconds to run on my 2021 M1 MacBook Pro.
    </p>

    <h2>Reflections</h2>

    <p>
        The authors of the paper set out and deliver on the very ambitious goal of building a controller that does not
        only
        prioritize safety but in fact guarantees it. They also verify other safety properties such as stability and
        robustness
        on more environments that were not covered here. However, their verification approach requires the very strong
        assumption that we have the precise system dynamics as a piecewise linear function. In addition to that the
        environments
        are simple enough that we can train a perfect oracle policy that can be distilled into a decision tree.
        It is therefore an open question what the boundaries of this approach are, i.e. at what point is the difference
        in DNN policy and DT just big enough so that we cannot distill a perfect DT?
    </p>

    <p>
        Overall, I greatly enjoyed studying and implementing this paper. The idea of extracting a DT from a DNN policy
        also has interesting implications for explainable RL agents (if the DT is small enough) and it would be
        interesting to see if this approach can be extended to more complex environments.
    </p>

</d-article>


<d-appendix>

    <d-footnote-list></d-footnote-list>
    <d-citation-list></d-citation-list>
</d-appendix>

<!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
<d-bibliography src="bibliography.bib"></d-bibliography>

</body>
